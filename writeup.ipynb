{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Cloning\n",
    "\n",
    "## Required Files\n",
    "### Are all required files submitted?\n",
    "The submission includes a model.py file, drive.py, model.h5, a writeup report and a running video.\n",
    "* model.py containing the script to create and train the model\n",
    "* drive.py for driving the car in autonomous mode\n",
    "* model.h5 containing a trained convolution neural network \n",
    "* writeup.ipynb\n",
    "* run1.mp4, a test1 with speed of 30mph.\n",
    "\n",
    "## Quality of Code\n",
    "### Is the code functional?\n",
    "The model provided can be used to successfully operate the simulation.\n",
    "I have modified the drive.py in order to chop the image from the simulator. \n",
    "I cut 40pixel at top and 20pixel at bottom.\n",
    "\n",
    "### Is the code usable and readable?\n",
    "The code is ver short and therefore easy to read, Despite of that I have added comments and references.\n",
    "\n",
    "\n",
    "## Model Architecture and Training Strategy\n",
    "### Has an appropriate model architecture been employed for the task?\n",
    "I have tryied a variety of models but eventually I have use the NVidia architectura but with an input of 100x320 since I have chop the image 40pixel top and 20pixel bottom. \n",
    "\n",
    "I don't use the last convolutional layer 64@1x18.\n",
    "\n",
    "The reference to the paper is:\n",
    "\n",
    "http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf\n",
    "\n",
    "Here is the code definning the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Preprocess incoming data, centered around zero with small standard deviation \n",
    "# From NVIDIA paper: http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf\n",
    "# In my case the input image is 120x320 not 66x220\n",
    "model.add(Lambda(lambda x: x/127.5 - 1.,\n",
    "        input_shape=( row, col,ch),\n",
    "        output_shape=(row, col, ch)))\n",
    "model.add(Conv2D(24, 5, 5, input_shape=(row, col, ch)))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add((Dropout(0.5)))\n",
    "model.add(Conv2D(36, 5, 5))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add((Dropout(0.5)))\n",
    "model.add(Conv2D(48, 3, 3))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add((Dropout(0.5)))\n",
    "model.add(Conv2D(64, 3, 3))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add((Dropout(0.5)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Has an attempt been made to reduce overfitting of the model?\n",
    "I use dropout after each convolutional layer.\n",
    "I use data augmentation in order to reduce overfitting, after reading this two blogs:\n",
    "\n",
    "* https://chatbotslife.com/using-augmentation-to-mimic-human-driving-496b569760a9#.zc82yp45t\n",
    "* https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "I use brightness, affine transform, only on X axis, flipped image and shadow augmentation.\n",
    "\n",
    "Here you can see the code, from Vivek Yadav:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From: https://chatbotslife.com/using-augmentation-to-mimic-human-driving-496b569760a9#.o92uic4yq\n",
    "# Data aumentation with brightness, shadow and transformations\n",
    "def augment_brightness_camera_images(image):\n",
    "    image1 = cv2.cvtColor(image,cv2.COLOR_RGB2HSV)\n",
    "    random_bright = .25+np.random.uniform()\n",
    "    #print(random_bright)\n",
    "    image1[:,:,2] = image1[:,:,2]*random_bright\n",
    "    image1 = cv2.cvtColor(image1,cv2.COLOR_HSV2RGB)\n",
    "    return image1\n",
    "def trans_image(image,steer,trans_range):\n",
    "    # Translation\n",
    "    cols=image.shape[1]\n",
    "    rows=image.shape[0]\n",
    "    tr_x = trans_range*np.random.uniform()-trans_range/2\n",
    "    steer_ang = steer + tr_x/trans_range*2*.2\n",
    "    tr_y = 40*np.random.uniform()-40/2\n",
    "    tr_y = 0\n",
    "    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])\n",
    "    image_tr = cv2.warpAffine(image,Trans_M,(cols,rows))\n",
    "    return image_tr,steer_ang\n",
    "def add_random_shadow(image):\n",
    "    top_y = 320*np.random.uniform()\n",
    "    top_x = 0\n",
    "    bot_x = 160\n",
    "    bot_y = 320*np.random.uniform()\n",
    "    image_hls = cv2.cvtColor(image,cv2.COLOR_RGB2HLS)\n",
    "    shadow_mask = 0*image_hls[:,:,1]\n",
    "    X_m = np.mgrid[0:image.shape[0],0:image.shape[1]][0]\n",
    "    Y_m = np.mgrid[0:image.shape[0],0:image.shape[1]][1]\n",
    "    shadow_mask[((X_m-top_x)*(bot_y-top_y) -(bot_x - top_x)*(Y_m-top_y) >=0)]=1\n",
    "    #random_bright = .25+.7*np.random.uniform()\n",
    "    if np.random.randint(2)==1:\n",
    "        random_bright = .5\n",
    "        cond1 = shadow_mask==1\n",
    "        cond0 = shadow_mask==0\n",
    "        if np.random.randint(2)==1:\n",
    "            image_hls[:,:,1][cond1] = image_hls[:,:,1][cond1]*random_bright\n",
    "        else:\n",
    "            image_hls[:,:,1][cond0] = image_hls[:,:,1][cond0]*random_bright    \n",
    "    image = cv2.cvtColor(image_hls,cv2.COLOR_HLS2RGB)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have the model parameters been tuned appropriately?\n",
    "Learning rate parameters are chosen with explanation, or an Adam optimizer is used.\n",
    "\n",
    "Here is the code where I use the optimizer, loss and metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the training data chosen appropriately?\n",
    "I have use the data from the course. In order to get robustness and reduce overffiting.\n",
    "I borrow a usb steering wheel in order to get data, but eventualy it has not been necessary since I have been able to keep the car on the track, even though trainning just with left, center and right images is more difficult.\n",
    "\n",
    "## Architecture and Training Documentation\n",
    "### Is the solution design documented?\n",
    "\n",
    "Firstly I started with a model from P2 sign recognition, made some modifications, added layers, but given that Nvidia did a good work and I was not as confident as I am now, I eventualy chose the Nvidia model but addapted to my image chopping.\n",
    "\n",
    "### Is the model architecture documented?\n",
    "I have used the Nvidia network with some differences.\n",
    "\n",
    "His is the original Nvidia model:\n",
    "\n",
    "![alt text](images/nvidianet.png \"Nvidia Neural Network\")\n",
    "\n",
    "There are two main differences on my model, the code of the model is above:\n",
    "* I use an input of 100x320 since I have chop the image 40pixel top and 20pixel bottom.\n",
    "* I don't use the last convolutional layer 64@1x18.\n",
    "\n",
    "Information of keras with the total numbers of parameters is:\n",
    "\n",
    "![alt text](images/kerasnet.png \"Keras Neural Network\")\n",
    "\n",
    "### Is the creation of the training dataset and training process documented?\n",
    "The trainning data set has been generated on the fly by data aumentation.\n",
    "\n",
    "Here it can seen the generator code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Used as a reference pointer so code always loops back around\n",
    "        sklearn.utils.shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            images = []\n",
    "            angles = []\n",
    "            for batch_sample in batch_samples:\n",
    "                side = np.random.randint(3)\n",
    "                if side==0:\n",
    "\t            #center\n",
    "                    name = './data/IMG/'+batch_sample[0].split('/')[-1]\n",
    "                    center_image = cv2.imread(name)\n",
    "                    center_angle = float(batch_sample[3])\n",
    "                    image=center_image\n",
    "                    angle=center_angle\n",
    "                if side==1:\n",
    "                    #left\n",
    "                    name = './data/IMG/'+batch_sample[1].split('/')[-1]\n",
    "                    left_image = cv2.imread(name)\n",
    "                    left_angle = float(batch_sample[3])+0.5\n",
    "                    image=left_image\n",
    "                    angle=left_angle\n",
    "                if side==2:\n",
    "                    #right\n",
    "                    name = './data/IMG/'+batch_sample[2].split('/')[-1]\n",
    "                    right_image = cv2.imread(name)\n",
    "                    right_angle = float(batch_sample[3])-0.5\n",
    "                    image=right_image\n",
    "                    angle=right_angle\n",
    "                image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "                flip=np.random.randint(5)\n",
    "                if flip==1:\n",
    "                    image = np.fliplr(image)\n",
    "                    angle = -angle\n",
    "                if flip==2:\n",
    "                    image,angle=trans_image(image,angle,100)\n",
    "                if flip==3:\n",
    "                    image = augment_brightness_camera_images(image)\n",
    "                if flip==4:\n",
    "                    image = add_random_shadow(image)\n",
    "                images.append(image)\n",
    "                angles.append(angle)\n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images,np.float32)\n",
    "            X_train = X_train[:,40:-20,:,:] \n",
    "            y_train = np.array(angles)\n",
    "            yield sklearn.utils.shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above augment the data in a number os ways.\n",
    "I show a image for every type of data augmentation:\n",
    "* flip: flip the image in order to balance movement.\n",
    "![alt text](images/data_augmentation/flip.png \"flip data augmentation\")\n",
    "* shift_left: shift image to the left in order to do a kind of recovery data\n",
    "![alt text](images/data_augmentation/shift_left.png \"shift_left data augmentation\")\n",
    "* shift_right: shift image to the right in order to do a kind of recovery data\n",
    "![alt text](images/data_augmentation/shift_right.png \"shift_right data augmentation\")\n",
    "* brightness: change brightness in order to make it brightness data invariant\n",
    "![alt text](images/data_augmentation/brightness.png \"brightness data augmentation\")\n",
    "* shadow: add shadow to image in order to make it shadow data invariant\n",
    "![alt text](images/data_augmentation/shadow.png \"shadow data augmentation\")\n",
    "* usual: just usual image from the dataset\n",
    "![alt text](images/data_augmentation/usual.png \"usual data augmentation\")\n",
    "\n",
    "This data augmentation has been paramount in order to make the neuralnetwork work properly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the best model I have done a kind of earlystoping, since I have used a callback in the fitting proccess in order to choose the best model generated. I have deffined a ModelCheckpoint that saves the best model up to the actual epoch.\n",
    "\n",
    "The best model is:\n",
    "\n",
    "model-checkpoint-63-0.16.hdf5\n",
    "\n",
    "A model with accuracy 0.16 at epoch 63, I ran the model for 100 epochs.\n",
    "\n",
    "Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"model-checkpoint-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit_generator(train_generator, \n",
    "                    samples_per_epoch= len(train_samples)*3, \n",
    "                    validation_data=validation_generator, \n",
    "                    nb_val_samples=len(validation_samples),\n",
    "                    callbacks=callbacks_list, \n",
    "                    nb_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "### Is the car able to navigate correctly on test data?\n",
    "The model is able to drive all the test1 track in full speed 30mph, with no issues.\n",
    "\n",
    "I uploaded a video where it can be seen how well the model perform.\n",
    "\n",
    "\n",
    "## Suggestions to Make Your Project Stand Out!\n",
    "### Track Two\n",
    "Since I am running out of time I have not been able to generalize more the model in order to work on test2 track.\n",
    "\n",
    "I am planning to make a autonomous RC car with real data, in a short time.\n",
    "\n",
    "But for the moment I am going to finish P5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
